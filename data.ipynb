{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from openbabel import pybel\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import concurrent.futures\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout_stderr():\n",
    "    # 打开空设备\n",
    "    with open(os.devnull, 'w') as fnull:\n",
    "        stdout, stderr = sys.stdout, sys.stderr\n",
    "        sys.stdout, sys.stderr = fnull, fnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout, sys.stderr = stdout, stderr\n",
    "\n",
    "def calculate_fingerprint(cif_path, hof_name):\n",
    "    \"\"\"\n",
    "    计算分子指纹，返回指纹位向量列表\n",
    "    \"\"\"\n",
    "    print(f\"开始计算指纹: {hof_name}\")\n",
    "    with suppress_stdout_stderr():\n",
    "        mol = next(pybel.readfile(\"cif\", cif_path))\n",
    "        fp = mol.calcfp(fptype='FP2')  # 使用FP2指纹\n",
    "\n",
    "    bits_fp = [0] * 1024  # 1024位的位向量\n",
    "    for bit in fp.bits:\n",
    "        if bit < 1024:  # 确保 bit 不超过索引范围\n",
    "            bits_fp[bit] = 1\n",
    "    print(f\"完成计算指纹: {hof_name}\")\n",
    "    return bits_fp\n",
    "\n",
    "def worker(cif_path, hof_name):\n",
    "    \"\"\"\n",
    "    Worker 函数用于计算分子指纹，不做超时控制。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(cif_path):\n",
    "        print(f\"文件不存在: {cif_path}\")\n",
    "        return hof_name, None\n",
    "    \n",
    "    try:\n",
    "        fingerprint = calculate_fingerprint(cif_path, hof_name)\n",
    "        return hof_name, fingerprint\n",
    "    except Exception as e:\n",
    "        print(f\"计算分子指纹时出错 ({hof_name}): {e}\")\n",
    "        return hof_name, None\n",
    "\n",
    "def update_fingerprints(input_file, new_keys_file, output_file, timeout=5):\n",
    "    \"\"\"\n",
    "    更新现有分子指纹 JSON 文件，将新的分子指纹追加到文件中。\n",
    "    \"\"\"\n",
    "    # 加载现有的分子指纹数据\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r') as f:\n",
    "            fingerprints = json.load(f)\n",
    "    else:\n",
    "        fingerprints = {}\n",
    "\n",
    "    # 读取新 JSON 文件中的 key 列表\n",
    "    with open(new_keys_file, 'r') as f:\n",
    "        new_keys_data = json.load(f)\n",
    "    new_keys = set(new_keys_data.keys())\n",
    "    \n",
    "    # 找出需要计算的文件（即新文件中存在，但不在旧指纹文件中的文件）\n",
    "    keys_to_process = new_keys - set(fingerprints.keys())\n",
    "    \n",
    "    # 并行计算新的分子指纹\n",
    "    def collect_result(result):\n",
    "        hof_name, fingerprint = result\n",
    "        if fingerprint is not None:\n",
    "            fingerprints[hof_name] = fingerprint\n",
    "\n",
    "    with multiprocessing.Pool(processes=4) as pool:\n",
    "        results = []\n",
    "        for hof_name in keys_to_process:\n",
    "            cif_path = f'/data/user2/wty/HOF/moftransformer/data/HOF_solvent/cifs/{hof_name}.cif'\n",
    "            result = pool.apply_async(worker, (cif_path, hof_name), callback=collect_result)\n",
    "            results.append(result)\n",
    "\n",
    "        # 等待所有任务完成\n",
    "        for result in results:\n",
    "            try:\n",
    "                result.get(timeout=timeout)\n",
    "            except multiprocessing.TimeoutError:\n",
    "                print(f\"计算超时: {result}\")\n",
    "    \n",
    "    # 保存更新后的分子指纹数据\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(fingerprints, f, indent=4)\n",
    "    print(f\"分子指纹已更新并保存到 {output_file}\")\n",
    "\n",
    "# 使用示例\n",
    "existing_fp_file = \"/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/all_fp.json\"  # 现有分子指纹 JSON 文件\n",
    "new_keys_file = \"/data/user2/wty/HOF/moftransformer/data/HOF_time/foldall/train_time.json\"  # 包含新键的 JSON 文件\n",
    "output_file = existing_fp_file  # 将输出文件路径设置为与现有文件相同\n",
    "\n",
    "update_fingerprints(existing_fp_file, new_keys_file, output_file, timeout=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def run_command_on_cif_files(folder_path, command_template, json_file_path):\n",
    "    # 读取JSON文件中的所有key值\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        cif_keys = set(json.load(f).keys())  # 获取所有的 key\n",
    "    \n",
    "    # 遍历指定文件夹中的所有 .cif 文件\n",
    "    for root, _, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            # 检查文件是否以 .cif 结尾，并且文件名在 cif_keys 中\n",
    "            if file.endswith('.cif') and file.rsplit('.', 1)[0] in cif_keys:\n",
    "                cif_file_path = Path(root) / file\n",
    "                # 构造命令\n",
    "                command = command_template.format(cif_file_path=cif_file_path)\n",
    "                # 运行命令\n",
    "                subprocess.run(command, shell=True)\n",
    "                print(f\"运行命令: {command}\")\n",
    "\n",
    "# 示例调用\n",
    "folder_path = '/data/user2/wty/HOF/moftransformer/data/HOF_solvent/cifs'  # 替换为你的实际文件夹路径\n",
    "command_template = './network -vol 1.5 1.5 1000000 {cif_file_path}'\n",
    "json_file_path = '/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/hofs.json'  # 替换为实际的 JSON 文件路径\n",
    "\n",
    "run_command_on_cif_files(folder_path, command_template, json_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file created at /data/user2/wty/HOF/moftransformer/data/HOF_pretrain_new/all_tobacco_hbond.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "def remove_empty_lists_from_json(src_json_path, dest_json_path):\n",
    "    # 读取源 JSON 文件\n",
    "    with open(src_json_path, 'r') as src_file:\n",
    "        data = json.load(src_file)\n",
    "    \n",
    "    # 删除值为空列表的键值对\n",
    "    cleaned_data = {k: v for k, v in data.items() if v != []}\n",
    "    \n",
    "    # 将修改后的数据写入目标 JSON 文件\n",
    "    with open(dest_json_path, 'w') as dest_file:\n",
    "        json.dump(cleaned_data, dest_file, indent=4)\n",
    "    print(f\"已删除空列表的键值对，并保存到 {dest_json_path}\")\n",
    "\n",
    "class HbondExtractor:\n",
    "    def __init__(self, cifs_path):\n",
    "        self.cifs_path = cifs_path\n",
    "\n",
    "    def get_Hbond_lists(self, cif_id):\n",
    "        donors, hs, acceptors = [], [], []\n",
    "        lis_path = os.path.join(self.cifs_path, f\"{cif_id}.lis\")\n",
    "        # 假如没有lis文件直接返回空list\n",
    "        if not os.path.exists(lis_path):\n",
    "            print(f\"No LIS file found for CIF ID {cif_id}.\")\n",
    "            return donors, hs, acceptors\n",
    "        with open(lis_path, 'r') as file:\n",
    "            content = file.read()\n",
    "            # 找到\"H....Acceptor\"到\"Translation of ARU-Code to CIF and Equivalent Position Code\"之间的数据块\n",
    "            data_block_match = re.search(r\"(Nr Typ Res Donor.*?)(?=\\n[A-Z])\", content, re.DOTALL | re.MULTILINE)\n",
    "        if data_block_match:\n",
    "            data_block = data_block_match.group(0)\n",
    "            lines = data_block.splitlines()\n",
    "            for idx, line in enumerate(lines):\n",
    "                # 假如line中有？则直接跳过\n",
    "                if \"?\" in line:\n",
    "                    continue\n",
    "                line = re.sub(r'Intra', ' ', line)\n",
    "                # 把形如“数字*”的字串替换为“数字 ”\n",
    "                line = re.sub(r'\\d\\*', '1 ', line)\n",
    "                # 替换形如 \"_[a-z]\" 的后缀\n",
    "                line = re.sub(r'_[a-z*]', ' ', line)\n",
    "                line = re.sub(r'_[0-9*]', ' ', line)\n",
    "                line = re.sub(r'_', ' ', line)\n",
    "                line = re.sub(r'>', ' ', line)\n",
    "                line = re.sub(r'<', ' ', line)\n",
    "                columns = line.split()\n",
    "                if len(columns) > 1 and (columns[0].isdigit() or columns[0].startswith('**')) and columns[1].isdigit():  # 检查每行是否以数字开头\n",
    "                    # 提取“元素符号+数字”格式\n",
    "                    donor = re.search(r'[A-Za-z]+\\d+[A-Z]*$', columns[2])\n",
    "                    h = re.search(r'[A-Za-z]+\\d+[A-Z]*$', columns[3])\n",
    "                    acceptor = re.search(r'[A-Za-z]+\\d+[A-Z]*$', columns[4])\n",
    "                    # 将匹配到的结果添加到列表中, 并且donor不以C开头\n",
    "                    if donor and not donor.group().startswith('C'):\n",
    "                        donors.append((donor.group(), idx))\n",
    "                        if h:\n",
    "                            hs.append((h.group(), idx))\n",
    "                        if acceptor:\n",
    "                            acceptors.append((acceptor.group(), idx))\n",
    "            # 假如三个list 的长度不相等，则输出cif_id并打印错误信息\n",
    "            if len(donors) != len(acceptors):\n",
    "                print('donors:', donors)\n",
    "                print('hs:', hs)\n",
    "                print('acceptors:', acceptors)\n",
    "                print(f\"Error in {cif_id}: Donor, H, Acceptor lists have different lengths.\")\n",
    "        return donors, hs, acceptors\n",
    "\n",
    "    def get_atom_indices(self, cif_id, atoms):\n",
    "        cif_path = os.path.join(self.cifs_path, f\"{cif_id}.cif\")\n",
    "        # print(\"cif_path:\", cif_path)\n",
    "        # print(\"atoms:\", atoms)\n",
    "        if not os.path.exists(cif_path):\n",
    "            print(f\"No CIF file found for CIF ID {cif_id}.\")\n",
    "            return []\n",
    "        atom_indices = []\n",
    "        with open(cif_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            atom_block = False\n",
    "            atom_list_start_index = None\n",
    "            for idx, line in enumerate(lines):\n",
    "                if line.strip() == \"_atom_site_occupancy\":\n",
    "                    atom_block = True\n",
    "                    atom_list_start_index = idx + 1\n",
    "                elif atom_block and line.strip() == \"loop_\":\n",
    "                    break\n",
    "                elif atom_block:\n",
    "                    columns = line.split()\n",
    "                    if len(columns) > 1 and columns[0] in atoms:\n",
    "                        atom_indices.append(idx - atom_list_start_index)\n",
    "        return atom_indices\n",
    "    \n",
    "    def get_atom_binary_list(self, cif_id, atoms):\n",
    "        cif_path = os.path.join(self.cifs_path, f\"{cif_id}.cif\")\n",
    "        if not os.path.exists(cif_path):\n",
    "            print(f\"No CIF file found for CIF ID {cif_id}.\")\n",
    "            return []\n",
    "        \n",
    "        binary_list = []\n",
    "        with open(cif_path, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            atom_block = False\n",
    "            for line in lines:\n",
    "                if line.strip() == \"_atom_site_occupancy\":\n",
    "                    atom_block = True\n",
    "                elif atom_block and line.strip() == \"loop_\":\n",
    "                    break\n",
    "                elif atom_block:\n",
    "                    columns = line.split()\n",
    "                    if len(columns) > 1:\n",
    "                        if columns[1] in atoms:\n",
    "                            binary_list.append(1)\n",
    "                        else:\n",
    "                            binary_list.append(0)\n",
    "        return binary_list\n",
    "\n",
    "    def create_json_from_cifs(self, output_json_path):\n",
    "        hbond_data = {}\n",
    "        # 遍历文件夹中的所有 .cif 文件\n",
    "        for filename in os.listdir(self.cifs_path):\n",
    "            if filename.endswith(\".cif\"):\n",
    "                cif_id = os.path.splitext(filename)[0]\n",
    "                donors, hs, acceptors = self.get_Hbond_lists(cif_id)\n",
    "                # 将donors, hs, acceptors合并为一个列表，包含元素符号和行号\n",
    "                all_atoms = list(set(donors + hs + acceptors))\n",
    "                # print(\"all_atoms:\", all_atoms)\n",
    "                atom_symbols = [atom[0] for atom in all_atoms]\n",
    "                atom_indices = self.get_atom_indices(cif_id, atom_symbols)\n",
    "                hbond_data[cif_id] = atom_indices\n",
    "        # 将结果写入JSON文件\n",
    "        with open(output_json_path, 'w') as json_file:\n",
    "            json.dump(hbond_data, json_file, indent=4)\n",
    "        print(f\"JSON file created at {output_json_path}\")\n",
    "\n",
    "# 使用示例\n",
    "if __name__ == \"__main__\":\n",
    "    cifs_path = \"/data/user2/wty/HOF/MOFDiff/mofdiff/data/mof_models/mof_models/bwdb_hoff/hofchecker\"  # 替换为你的cif文件夹路径\n",
    "    output_json_path = \"/data/user2/wty/HOF/moftransformer/data/HOF_pretrain_new/fold8/all_tobacco_hbond.json\"  # 输出JSON文件的路径\n",
    "    extractor = HbondExtractor(cifs_path)\n",
    "    extractor.create_json_from_cifs(output_json_path)\n",
    "    \n",
    "    # src_json_path = Path('/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/all_tobacco_hbond.json')  # 替换为你的源 JSON 文件路径\n",
    "    # dest_json_path = Path('/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/all_tobacco_hbond.json')  # 替换为你的目标 JSON 文件路径\n",
    "    # remove_empty_lists_from_json(src_json_path, dest_json_path)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import ast\n",
    "\n",
    "# 文件路径\n",
    "csv_file_path = '/data/user2/wty/HOF/logs/HOF_pretrain/fold3/nh_na/pretrained_mof_seed0_from_pmtransformer/version_0/val_prediction.csv'\n",
    "keys_json_path = '/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/fold3/val_mtp.json'\n",
    "output_json_path = '/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/fold3/val_mtp.json'\n",
    "\n",
    "# 从CSV中读取mtp_logits并找出最大置信度的分类索引\n",
    "def extract_max_confidence_indices(csv_path):\n",
    "    max_indices = []\n",
    "    with open(csv_path, mode='r', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            logits = ast.literal_eval(row['mtp_logits'])  # 将字符串转换为列表\n",
    "            max_index = logits.index(max(logits))  # 获取最大值的索引\n",
    "            max_indices.append(max_index)\n",
    "    return max_indices\n",
    "\n",
    "# 读取keys并生成输出json\n",
    "def create_output_json(keys_path, max_indices, output_path):\n",
    "    with open(keys_path, 'r', encoding='utf-8') as keyfile:\n",
    "        keys_dict = json.load(keyfile)\n",
    "    if len(keys_dict) != len(max_indices):\n",
    "        raise ValueError(\"keys文件中的键数量与CSV文件中的行数不匹配\")\n",
    "    \n",
    "    keys = list(keys_dict.keys())\n",
    "    output_data = {keys[i]: max_indices[i] for i in range(len(keys))}\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "        json.dump(output_data, outfile, indent=4, ensure_ascii=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 读取CSV文件并提取最大置信度索引\n",
    "    max_indices = extract_max_confidence_indices(csv_file_path)\n",
    "    \n",
    "    # 生成输出JSON文件\n",
    "    create_output_json(keys_json_path, max_indices, output_json_path)\n",
    "    \n",
    "    print(f\"结果已保存到 {output_json_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def reset_values_to_zero(input_file, output_file):\n",
    "    # 读取原始JSON文件\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 将所有value置为0\n",
    "    new_data = {key: 0 for key in data.keys()}\n",
    "    \n",
    "    # 将新的数据写入输出文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(new_data, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(\"新的JSON文件已保存到\", output_file)\n",
    "\n",
    "# 示例用法\n",
    "input_file = '/data/user2/wty/HOF/ML_method/all_fp.json'\n",
    "output_file = '/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/fold2/mtp_train.json'\n",
    "reset_values_to_zero(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def update_json_with_csv(csv_file, json_file, output_file):\n",
    "    # 读取JSON文件中的key\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        json_data = json.load(f)\n",
    "    keys = list(json_data.keys())\n",
    "\n",
    "    # 读取CSV文件中的vfp_logits列\n",
    "    df = pd.read_csv(csv_file)\n",
    "    logits = df['vfp_logits'].tolist()\n",
    "\n",
    "    # 检查JSON和CSV文件的行数是否匹配\n",
    "    if len(keys) != len(logits):\n",
    "        print(\"错误: JSON文件中的键数量和CSV文件中的vfp_logits数量不匹配\")\n",
    "        return\n",
    "\n",
    "    # 将CSV中的vfp_logits作为新的value生成新的JSON数据\n",
    "    # 如果value 值为负数，则将其设置为0\n",
    "\n",
    "    updated_data = {key: (logits[i] if logits[i] >= 0 else 0.0) for i, key in enumerate(keys)}\n",
    "\n",
    "    # 保存新的JSON文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(updated_data, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"已保存新的JSON文件到 {output_file}\")\n",
    "\n",
    "# 示例用法\n",
    "csv_file = '/data/user2/wty/HOF/logs/HOF_pretrain/fold3/nh_na/pretrained_mof_seed0_from_pmtransformer/version_0/test_prediction.csv'  # CSV文件路径\n",
    "json_file = '/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/fold3/test_vfp.json'  # 原始JSON文件路径\n",
    "output_file = '/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/fold3/test_vfp.json'  # 生成的JSON文件路径\n",
    "\n",
    "update_json_with_csv(csv_file, json_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def replace_values_from_reference(reference_file, input_files, output_files, missing_keys_file):\n",
    "    # 加载参考JSON文件\n",
    "    with open(reference_file, 'r', encoding='utf-8') as f:\n",
    "        reference_data = json.load(f)\n",
    "\n",
    "    # 用于保存所有缺失的键\n",
    "    all_missing_keys = {}\n",
    "\n",
    "    # 遍历每个输入文件\n",
    "    for input_file, output_file in zip(input_files, output_files):\n",
    "        # 加载当前输入JSON文件\n",
    "        with open(input_file, 'r', encoding='utf-8') as f:\n",
    "            input_data = json.load(f)\n",
    "        \n",
    "        # 创建用于存储更新后的数据字典\n",
    "        updated_data = {}\n",
    "        missing_keys = []\n",
    "\n",
    "        # 遍历输入文件的每个key\n",
    "        for key in input_data.keys():\n",
    "            if key in reference_data:\n",
    "                # 如果参考JSON中存在该key，使用参考JSON的value\n",
    "                updated_data[key] = reference_data[key]\n",
    "            else:\n",
    "                # 如果参考JSON中不存在该key，记录为缺失\n",
    "                missing_keys.append(key)\n",
    "                all_missing_keys[key] = 0  # 将缺失的key保存到总的缺失字典中，value设置为0\n",
    "        \n",
    "        # 打印缺失的key\n",
    "        if missing_keys:\n",
    "            print(f\"{input_file} 缺失的key: {missing_keys}\")\n",
    "        \n",
    "        # 将更新后的数据保存到新的输出文件\n",
    "        with open(output_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(updated_data, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"已保存更新后的文件: {output_file}\")\n",
    "\n",
    "    # 将所有缺失的key保存到指定的缺失键JSON文件\n",
    "    with open(missing_keys_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(all_missing_keys, f, indent=4, ensure_ascii=False)\n",
    "    print(f\"缺失的key已保存到 {missing_keys_file}\")\n",
    "\n",
    "# 示例用法\n",
    "reference_file = '/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/all_fp.json'  # 参考JSON文件路径\n",
    "input_files = [\n",
    "    '/data/user2/wty/HOF/moftransformer/data/HOF_solvent/foldall/train_solvent.json'\n",
    "]  # 输入JSON文件路径列表\n",
    "output_files = [\n",
    "    \"/data/user2/wty/HOF/moftransformer/data/HOF_solvent/foldall/train_fp.json\"\n",
    "]  # 输出JSON文件路径列表\n",
    "missing_keys_file = '/data/user2/wty/HOF/moftransformer/data/HOF_solvent/foldall/missing_keys.json'  # 缺失key的JSON文件路径\n",
    "\n",
    "replace_values_from_reference(reference_file, input_files, output_files, missing_keys_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "\n",
    "def process_json_files(source_folder, destination_folder):\n",
    "    # 确保目标文件夹存在\n",
    "    os.makedirs(destination_folder, exist_ok=True)\n",
    "    \n",
    "    # 遍历 source_folder 中的所有文件\n",
    "    for filename in os.listdir(source_folder):\n",
    "        if filename.endswith(\".json\"):\n",
    "            file_path = os.path.join(source_folder, filename)\n",
    "            \n",
    "            # 读取 JSON 文件\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                data = json.load(file)\n",
    "            \n",
    "            # 删除以 \"sample\" 和 \"tobacco\" 开头的键值对\n",
    "            keys_to_delete = [key for key in data.keys() if key.startswith(\"sample\") or key.startswith(\"tobacco\")]\n",
    "            for key in keys_to_delete:\n",
    "                del data[key]\n",
    "            \n",
    "            # 将修改后的数据写回 JSON 文件\n",
    "            with open(file_path, 'w', encoding='utf-8') as file:\n",
    "                json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "            \n",
    "            # 移动修改后的文件到目标文件夹\n",
    "            shutil.move(file_path, os.path.join(destination_folder, filename))\n",
    "            print(f\"Processed and moved file: {filename}\")\n",
    "\n",
    "# 使用示例\n",
    "source_folder = \"/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/fold2\"\n",
    "destination_folder = \"/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/fold3\"\n",
    "process_json_files(source_folder, destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import multiprocessing\n",
    "import json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from openbabel import pybel\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import classification_report\n",
    "import warnings\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import concurrent.futures\n",
    "\n",
    "@contextmanager\n",
    "def suppress_stdout_stderr():\n",
    "    # 打开空设备\n",
    "    with open(os.devnull, 'w') as fnull:\n",
    "        stdout, stderr = sys.stdout, sys.stderr\n",
    "        sys.stdout, sys.stderr = fnull, fnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout, sys.stderr = stdout, stderr\n",
    "\n",
    "def calculate_fingerprint(cif_path, hof_name):\n",
    "    \"\"\"\n",
    "    计算分子指纹，返回指纹位向量列表\n",
    "    \"\"\"\n",
    "    print(f\"开始计算指纹: {hof_name}\")\n",
    "    with suppress_stdout_stderr():\n",
    "        mol = next(pybel.readfile(\"cif\", cif_path))\n",
    "        fp = mol.calcfp(fptype='FP2')  # 使用FP2指纹\n",
    "\n",
    "    bits_fp = [0] * 1024  # 1024位的位向量\n",
    "    for bit in fp.bits:\n",
    "        if bit < 1024:  # 确保 bit 不超过索引范围\n",
    "            bits_fp[bit] = 1\n",
    "    print(f\"完成计算指纹: {hof_name}\")\n",
    "    return bits_fp\n",
    "\n",
    "def worker(cif_path, hof_name):\n",
    "    \"\"\"\n",
    "    Worker 函数用于计算分子指纹，不做超时控制。\n",
    "    \"\"\"\n",
    "    if not os.path.exists(cif_path):\n",
    "        print(f\"文件不存在: {cif_path}\")\n",
    "        return hof_name, None\n",
    "    \n",
    "    try:\n",
    "        fingerprint = calculate_fingerprint(cif_path, hof_name)\n",
    "        return hof_name, fingerprint\n",
    "    except Exception as e:\n",
    "        print(f\"计算分子指纹时出错 ({hof_name}): {e}\")\n",
    "        return hof_name, None\n",
    "\n",
    "def update_fingerprints(input_file, new_keys_file, output_file, timeout=5):\n",
    "    \"\"\"\n",
    "    更新现有分子指纹 JSON 文件，将新的分子指纹追加到文件中。\n",
    "    \"\"\"\n",
    "    # 加载现有的分子指纹数据\n",
    "    if os.path.exists(output_file):\n",
    "        with open(output_file, 'r') as f:\n",
    "            fingerprints = json.load(f)\n",
    "    else:\n",
    "        fingerprints = {}\n",
    "\n",
    "    # 读取新 JSON 文件中的 key 列表\n",
    "    with open(new_keys_file, 'r') as f:\n",
    "        new_keys_data = json.load(f)\n",
    "    new_keys = set(new_keys_data.keys())\n",
    "    \n",
    "    # 找出需要计算的文件（即新文件中存在，但不在旧指纹文件中的文件）\n",
    "    keys_to_process = new_keys - set(fingerprints.keys())\n",
    "    \n",
    "    # 并行计算新的分子指纹\n",
    "    def collect_result(result):\n",
    "        hof_name, fingerprint = result\n",
    "        if fingerprint is not None:\n",
    "            fingerprints[hof_name] = fingerprint\n",
    "\n",
    "    with multiprocessing.Pool(processes=40) as pool:\n",
    "        results = []\n",
    "        for hof_name in keys_to_process:\n",
    "            cif_path = f'/data/user2/wty/HOF/moftransformer/data/HOF_solvent/cifs/{hof_name}.cif'\n",
    "            if not os.path.exists(cif_path):\n",
    "                print(f\"文件不存在: {cif_path}\")\n",
    "                continue\n",
    "            result = pool.apply_async(worker, (cif_path, hof_name), callback=collect_result)\n",
    "            results.append(result)\n",
    "\n",
    "        # 等待所有任务完成\n",
    "        for result in results:\n",
    "            try:\n",
    "                result.get(timeout=timeout)\n",
    "            except multiprocessing.TimeoutError:\n",
    "                print(f\"计算超时: {result}\")\n",
    "    \n",
    "    # 保存更新后的分子指纹数据\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(fingerprints, f, indent=4)\n",
    "    print(f\"分子指纹已更新并保存到 {output_file}\")\n",
    "\n",
    "# 使用示例\n",
    "existing_fp_file = \"/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/all_fp.json\"  # 现有分子指纹 JSON 文件\n",
    "new_keys_file = \"/data/user2/wty/HOF/moftransformer/data/HOF_temperature/foldfake/test_temperature.json\"  # 包含新键的 JSON 文件\n",
    "output_file = existing_fp_file  # 将输出文件路径设置为与现有文件相同\n",
    "\n",
    "update_fingerprints(existing_fp_file, new_keys_file, output_file, timeout=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def delete_keys_from_json(target_json_path, specified_json_path):\n",
    "    # 读取目标 JSON 文件，获取需要删除的键\n",
    "    with open(target_json_path, 'r', encoding='utf-8') as target_file:\n",
    "        target_data = json.load(target_file)\n",
    "        keys_to_delete = set(target_data.keys())  # 将目标 JSON 文件中的键存储为集合\n",
    "    \n",
    "    # 读取指定的 JSON 文件\n",
    "    with open(specified_json_path, 'r', encoding='utf-8') as specified_file:\n",
    "        specified_data = json.load(specified_file)\n",
    "    \n",
    "    # 从指定 JSON 文件中删除目标 JSON 文件中的键\n",
    "    for key in keys_to_delete:\n",
    "        if key in specified_data:\n",
    "            del specified_data[key]\n",
    "    \n",
    "    # 将修改后的数据写回指定的 JSON 文件\n",
    "    with open(specified_json_path, 'w', encoding='utf-8') as specified_file:\n",
    "        json.dump(specified_data, specified_file, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    print(f\"已删除 {len(keys_to_delete)} 个键值对，结果已保存到 {specified_json_path}\")\n",
    "\n",
    "# 使用示例\n",
    "target_json_path = '/data/user2/wty/HOF/moftransformer/data/HOF_solvent/foldall/missing_keys.json'      # 包含要删除的键的目标 JSON 文件\n",
    "specified_json_path = '/data/user2/wty/HOF/moftransformer/data/HOF_solvent/foldall/train_solvent.json' # 指定的 JSON 文件\n",
    "delete_keys_from_json(target_json_path, specified_json_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def extract_fp(reference_file, fp_all_file, output_file):\n",
    "    # 加载参考文件\n",
    "    with open(reference_file, 'r', encoding='utf-8') as ref_file:\n",
    "        ref_data = json.load(ref_file)\n",
    "\n",
    "    # 加载 fp_all 文件\n",
    "    with open(fp_all_file, 'r', encoding='utf-8') as fp_file:\n",
    "        fp_data = json.load(fp_file)\n",
    "\n",
    "    # 初始化输出数据\n",
    "    output_data = {}\n",
    "\n",
    "    # 遍历参考文件中的键并从 fp_all 中提取对应的键值对\n",
    "    for key in ref_data.keys():\n",
    "        if key in fp_data:\n",
    "            output_data[key] = fp_data[key]\n",
    "        else:\n",
    "            print(f\"Error: Key '{key}' not found in {fp_all_file}.\")\n",
    "\n",
    "    # 保存提取的键值对到输出文件\n",
    "    with open(output_file, 'w', encoding='utf-8') as out_file:\n",
    "        json.dump(output_data, out_file, ensure_ascii=False, indent=4)\n",
    "    print(f\"提取结果已保存到 {output_file}\")\n",
    "\n",
    "def main():\n",
    "    base_folder = \"/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/fold2\"  # 目标文件夹路径\n",
    "    fp_all_file = '/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/all_fp.json'\n",
    "\n",
    "    # 定义文件路径\n",
    "    files = {\n",
    "        \"train\": \"/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/fold2/train_mtp.json\",\n",
    "        \"test\": \"/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/fold2/test_mtp.json\",\n",
    "        \"val\": \"/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/fold2/val_mtp.json\"\n",
    "    }\n",
    "\n",
    "    # 对每个文件执行提取操作\n",
    "    for file_type, filename in files.items():\n",
    "        reference_file = os.path.join(base_folder, filename)\n",
    "        output_file = os.path.join(base_folder, f\"{file_type}_fp.json\")\n",
    "        extract_fp(reference_file, fp_all_file, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def count_key_value_pairs(json_path):\n",
    "    # 读取 JSON 文件\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    # 计算键值对数量\n",
    "    num_pairs = len(data)\n",
    "    \n",
    "    print(f\"JSON 文件中有 {num_pairs} 个键值对。\")\n",
    "\n",
    "# 示例调用\n",
    "json_path = Path('/data/user2/wty/HOF/moftransformer/data/HOF_pretrain_new/all_fp.json')  # 替换为你的 JSON 文件路径\n",
    "\n",
    "count_key_value_pairs(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def split_json(json_path, train_path, val_path, test_path, train_ratio=0.67, val_ratio=0.17, test_ratio=0.16):\n",
    "    # 读取 JSON 文件\n",
    "    with open(json_path, 'r', encoding='utf-8') as json_file:\n",
    "        data = json.load(json_file)\n",
    "    \n",
    "    # 获取所有键值对\n",
    "    items = list(data.items())\n",
    "    \n",
    "    # 打乱顺序\n",
    "    random.shuffle(items)\n",
    "    \n",
    "    # 计算每个集合的大小\n",
    "    total = len(items)\n",
    "    train_size = int(total * train_ratio)\n",
    "    val_size = int(total * val_ratio)\n",
    "    test_size = total - train_size - val_size\n",
    "    \n",
    "    # 划分数据集\n",
    "    train_items = items[:train_size]\n",
    "    val_items = items[train_size:train_size + val_size]\n",
    "    test_items = items[train_size + val_size:]\n",
    "    \n",
    "    # 转换为字典\n",
    "    train_data = dict(train_items)\n",
    "    val_data = dict(val_items)\n",
    "    test_data = dict(test_items)\n",
    "    \n",
    "    # 写入 JSON 文件\n",
    "    with open(train_path, 'w', encoding='utf-8') as train_file:\n",
    "        json.dump(train_data, train_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    with open(val_path, 'w', encoding='utf-8') as val_file:\n",
    "        json.dump(val_data, val_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    with open(test_path, 'w', encoding='utf-8') as test_file:\n",
    "        json.dump(test_data, test_file, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"数据集已拆分并保存到 {train_path}, {val_path}, {test_path}\")\n",
    "\n",
    "# 示例调用\n",
    "json_path = '/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/av_volume_fractions.json'  # 替换为你的输入 JSON 文件路径\n",
    "train_path = '/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/fold3/train_vfp.json'  # 替换为你的输出 train JSON 文件路径\n",
    "val_path = '/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/fold3/val_vfp.json'  # 替换为你的输出 val JSON 文件路径\n",
    "test_path = '/data/user2/wty/HOF/moftransformer/data/HOF_pretrain/fold3/test_vfp.json'  # 替换为你的输出 test JSON 文件路径\n",
    "\n",
    "split_json(json_path, train_path, val_path, test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "def get_graph(cif_id, cifs_path):\n",
    "    file_graph = os.path.join(cifs_path, f\"{cif_id}.graphdata\")\n",
    "\n",
    "    graphdata = pickle.load(open(file_graph, \"rb\"))\n",
    "    # graphdata = [\"cif_id\", \"atom_num\", \"nbr_idx\", \"nbr_dist\", \"uni_idx\", \"uni_count\"]\n",
    "    atom_num = torch.LongTensor(graphdata[1].copy())\n",
    "    # print(\"atom_num:\", atom_num)\n",
    "    nbr_idx = torch.LongTensor(graphdata[2].copy()).view(len(atom_num), -1)\n",
    "    uni_idx = graphdata[4]\n",
    "    uni_count = graphdata[5]\n",
    "\n",
    "    return {\n",
    "        \"atom_num\": atom_num,\n",
    "        \"nbr_idx\": nbr_idx,\n",
    "        \"uni_idx\": uni_idx,\n",
    "        \"uni_count\": uni_count,\n",
    "    }\n",
    "cifs_path = \"/data/user2/wty/HOF/MOFDiff/mofdiff/data/mof_models/mof_models/bwdb_hoff/hofchecker/total\"\n",
    "cif_id = \"sample_822_47\"\n",
    "result = get_graph(cif_id, cifs_path)\n",
    "print(len(result[\"atom_num\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "source_json = '/data/user2/wty/HOF/moftransformer/data/HOF_pretrain_new/fold4/train_hbond.json'\n",
    "des_json = '/data/user2/wty/HOF/moftransformer/data/HOF_pretrain_new/fold7/train_hbond.json'\n",
    "\n",
    "source_data = json.load(open(source_json, 'r'))\n",
    "des_data = json.load(open(des_json, 'r'))\n",
    "\n",
    "for key in source_data.keys():\n",
    "    if key not in des_data:\n",
    "        des_data[key] = source_data[key]\n",
    "\n",
    "with open(des_json, 'w', encoding='utf-8') as f:\n",
    "    json.dump(des_data, f, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cdvae_wty",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
