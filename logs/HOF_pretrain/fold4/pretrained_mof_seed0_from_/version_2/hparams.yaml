config: !!python/object/new:sacred.config.config_summary.ConfigSummary
  dictitems:
    __doc__: '

      # prepare_data

      max_num_atoms = 300

      min_length = 30

      max_length = 60

      radius = 8

      max_nbr_atoms = 12

      '
    accelerator: auto
    atom_fea_len: 64
    batch_size: 8
    cifs_path: /mnt/user2/wty/HOF/MOFDiff/mofdiff/data/mof_models/mof_models/bwdb_hoff/hofchecker/total
    decay_power: 1
    devices:
    - 5
    downstream: vfp
    draw_false_grid: false
    drop_rate: 0.1
    early_stop_patience: 100
    end_lr: 0
    exp_name: pretrained_mof
    fold: fold4
    hbond: false
    hid_dim: 768
    img_size: 30
    in_chans: 1
    learning_rate: 1.0e-05
    load_path: ''
    log_dir: ./logs/HOF_pretrain/fold4
    loss_names:
      bbc: 0
      classification: 0
      fp: 1
      ggm: 0
      hbond: 1
      moc: 0
      mpp: 0
      mtp: 0
      regression: 0
      solvent_classification: 0
      vfp: 1
    lr_mult: 1
    max_epochs: 2000
    max_graph_len: 300
    max_grid_len: -1
    max_nbr_atoms: 12
    max_steps: -1
    mean: null
    mlp_ratio: 4
    mpp_ratio: 0.15
    n_classes: 0
    nbr_fea_len: 64
    num_heads: 12
    num_layers: 12
    num_nodes: 1
    num_workers: 16
    optim_type: adamw
    patch_size: 5
    per_gpu_batchsize: 8
    precision: 16
    resume_from: null
    root_dataset: ./data/HOF_pretrain
    seed: 0
    std: null
    test_only: false
    val_check_interval: 1.0
    visualize: false
    warmup_steps: 0.05
    weight_decay: 0.01
  state:
    added: !!set {}
    docs:
      atom_fea_len: 'max_supercell_atoms = None  # number of maximum atoms in supercell
        atoms'
      batch_size: desired batch size; for gradient accumulation
      downstream: downstream
      exp_name: model
      hbond: hbond
      hid_dim: transformer setting
      img_size: grid setting
      in_chans: channels of grid image
      lr_mult: multiply lr for downstream heads
      max_graph_len: number of maximum nodes in graph
      max_grid_len: when -1, max_image_len is set to maximum ph*pw of batch images
      max_steps: num_data * max_epoch // batch_size (accumulate_grad_batches)
      mean: normalization target
      num_workers: the number of cpu's core
      optim_type: adamw, adam, sgd (momentum=0.9)
      patch_size: length of patch
      per_gpu_batchsize: you should define this manually with per_gpu_batch_size
      resume_from: PL Trainer Setting
      root_dataset: below params varies with the environment
      seed: the random seed for this experiment
      visualize: return attention map
      warmup_steps: int or float ( max_steps * warmup_steps)
    ignored_fallbacks: !!set {}
    modified: !!set {}
    typechanged: {}
